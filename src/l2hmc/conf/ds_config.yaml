---
dump_state: false
gradient_accumulation_steps: 1
# prescale_gradients: false
# wall_clock_breakdown: false
# zero_allow_untested_optimizer: true
# optimizer:
#   type: OneBitAdam
#   params:
#     lr: 0.0004
optimizer:
  type: AdamW
  params:
    lr: 0.001
# flops_profiler:
#   enabled: false
#   profile_step: 1
#   module_depth: -1
#   top_modules: 1
#   detailed: false
#   output_file: null
fp16:
  enabled: true
  # min_loss_scale: 0
# bf16:
#   enabled: false
# comms_logger:
#   enabled: false
#   verbose: false
#   prof_all: false
#   debug: false
wandb:
  enabled: true
# autotuning:
#   enabled: false
#   arg_mappings:
#     train_micro_batch_size_per_gpu: --per_device_train_batch_size
#     gradient_accumulation_steps: --gradient_accumulation_steps
# zero_optimization:
#   stage: 0
#   allgather_partitions: true
#   reduce_scatter: true
#   allgather_bucket_size: 5e8
#   reduce_bucket_size: 5e8
#   overlap_comm: true
#   contiguous_gradients: true
#   offload_param:
#     device: nvme
#     nvme_path: /raid/scratch
#   offload_optimizer:
#     device: cpu
#     pin_memory: true
...
