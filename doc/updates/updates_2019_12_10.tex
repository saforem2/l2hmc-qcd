\section{Updates 12/10/2019}%
\label{sec:updates2019_12_10}
%
\begin{itemize}
  \item It was observed that if the model was trained (using simulated
    annealing) to a final $\beta_{\mathrm{final}}$ that was greater than the
    value of $\beta_{\mathrm{inference}}$ at which we intend to run inference,
    the bias in the average plaquette decreased. For example, if
    $\beta_{\mathrm{final}} = 5$ and $\beta_{\mathrm{inference}} = 4$, the bias
    seemed to be smaller when running at $\beta_{\mathrm{inference}} = 4$ than
    it was when running at $\beta_{\mathrm{inference}} = 5$.
    \begin{itemize}
      \item Try using larger value of $\beta_{\mathrm{final}}$ in annealing
        schedule to test if this is consistent.
    \end{itemize}
  \item Reduce $N_{\mathrm{LF}} \rightarrow 1$
  \item Split $\texttt{net weights}(= [\alpha_{S}, \alpha_{T}, \alpha_{Q}])$
    into separate components for $x$ and $v$ so that 
    \begin{equation}
      \texttt{net weights} \equiv [\alpha_{S_x}, \alpha_{T_x}, \alpha_{Q_x},
      \alpha_{S_v}, \alpha_{T_v}, \alpha_{Q_v}]
    \end{equation}
  \item Explicitly loop over different values of $\texttt{net weights}$ to
    determine which (if any) of the functions $S_x, T_x, Q_x, S_v, T_v$, or
    $Q_v$ has the largest contribution to the bias in the average plaquette.
  \item Slowly turn things off until results agree with generic HMC and then
    turn them back on individually until difference reappears. 
  \item Looking at summaries in TensorBoard, it was observed that certain
    gradients experienced large spikes during training
    (See Fig.~\ref{fig:gradient_spikes}).
    \begin{itemize}
      \item \textbf{Use gradient clipping!} (by global norm)
    \end{itemize}
\end{itemize}
%
\begin{figure}%
  \centering
  \includegraphics[width=\textwidth]{updates_2019_12_10/gradient_spikes}%
  \caption{Example of a spiking gradient.}%
  \label{fig:gradient_spikes}
\end{figure}
%
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{updates_2019_12_10/plaqs_diffs_steps5000_lf5}%
  \caption{Trace plots and histograms from an inference run with
  $N_{\mathrm{LF}} = 5$ and various values of $\texttt{net weights}$.}%
  \label{fig:trace_hist_lf5}
\end{figure}
%
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{updates_2019_12_10/plaqs_diffs_steps5000_lf4}%
  \caption{Trace plots and histograms from an inference run with
  $N_{\mathrm{LF}} = 4$ and various values of $\texttt{net weights}$.}%
  \label{fig:trace_hist_lf4}
\end{figure}
%
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{updates_2019_12_10/plaqs_diffs_steps5000_lf3_3}%
  \caption{Trace plots and histograms from an inference run with
  $N_{\mathrm{LF}} = 3$ and various values of $\texttt{net weights}$.}
\end{figure}
%
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{updates_2019_12_10/plaqs_diffs_steps5000_lf2}%
  \caption{Trace plots and histograms from an inference run with
  $N_{\mathrm{LF}} = 2$ and various values of $\texttt{net weights}$.}
\end{figure}
%
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{updates_2019_12_10/plaqs_diffs_steps2000_lf1_1}%
  \caption{Trace plots and histograms of $\delta \phi_{p}$ from an inference
  run with $N_{\mathrm{LF}} = 1$ and various values of $\texttt{net weights}$.}
\end{figure}
%
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{updates_2019_12_10/xdiffs/xdiff_lf1_1}%
  \caption{Trace plots and histograms of $\delta x = x^{(i+1)} -
    x^{(i)} \mod{2\pi} $ from an inference run with $N_{\mathrm{LF}} = 1$ and various
    values of $\texttt{net weights}$.}
\end{figure}

\clearpage%
