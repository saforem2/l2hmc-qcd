%\begin{document}
\section{Updates: 09/22/2020}%
\label{sec:updates_2020_09_22}
\subsection{Single Network}%
\label{subsec:single_network}
%
Recall that a single leapfrog step (in the \emph{forward} \((d = +1)\)
direction) of the L2HMC algorithm consists of the following 4 updates:
% \vspace{-20pt}
\begin{enumerate}
  \item \(\mathrm{\textbf{vNet}}: \zeta_{1} \coloneqq (x, \partial_{x}U(x), t)
    \longrightarrow (S_{v}, T_{v}, Q_{v})\):
    \begin{equation}
      \vp = v \odot \exp{\left(\frac{\eps}{2}S_{v}(\zeta_{1})\right)}% 
             - \frac{\eps}{2}\left[\partial_{x}\,U(x)\odot \exp{\left(\eps
               Q_{v}(\zeta_{1})\right)}%
             + T_{v}(\zeta_{1})\right]
    \end{equation}
  \item \(\mathrm{\mathbf{xNet}}: \zeta_{2} \coloneqq 
    (x_{\bar{m}^{t}}, \vp, t) \longrightarrow (S_{x}, T_{x}, Q_{x})\):
    \begin{equation}
      \xp = x_{\bar{m}^{t}} + m^{t}\odot \left[x \odot \exp{\left(\eps
              S_{x}(\zeta_{2})\right)}%
            + \eps\left(\vp\odot\exp{\left(\eps Q_{x}(\zeta_{2})\right)} 
            + T_{x}(\zeta_{2})\right)\right]
    \end{equation}
  \item \(\mathrm{\mathbf{xNet}}: \zeta_{3} \coloneqq (x_{m^{t}}^{\prime}, \vp,
    t) \longrightarrow (S_{x}, T_{x}, Q_{x})\):
    \begin{equation}
      \xpp = x^{\prime}_{m^{t}} + \bar{m}^{t}\odot \left[\xp \odot \exp{\left(\eps
              S_{x}(\zeta_{3})\right)}%
            + \eps\left(\vp\odot\exp{\left(\eps Q_{x}(\zeta_{3})\right)}
            + T_{x}(\zeta_{3})\right)\right]
    \end{equation}
  \item \(\mathrm{\mathbf{vNet}}: \zeta_{4} \coloneqq (\xpp,
    \partial_{x}U(\xpp), t) \longrightarrow (S_{v}, T_{v}, Q_{v})\):
    \begin{equation}
      \vpp = \vp \odot \exp{\left(\frac{\eps}{2}S_{v}(\zeta_{4})\right)}%
              - \frac{\eps}{2}\left[\partial_{x}\,U(\xpp)\odot \exp{\left(\eps
                  Q_{v}(\zeta_{4})\right)}%
              + T_{v}(\zeta_{4})\right].
    \end{equation}
\end{enumerate}
%

A complete trajectory (of \(\Nlf\) leapfrog steps) is then obtained via
\(\Nlf\) successive applications of the above procedure, followed by a MH
accept/reject step.

Note that in this approach we use the \textbf{same two networks for all
leapfrog steps}, namely: \(\{\mathrm{\mathbf{xNet}}, \mathrm{\mathbf{vNet}}\}\),
which update \(\{x, v\}\) respectively.
%
\subsection{Separate Networks}%
\label{subsec:separate_networks}
%
This can be generalized by introducing \textbf{different networks for each
leapfrog step} (e.g.\@ the first LF step uses \(\{\xNet^{(1)},
\vNet^{(1)}\}\), the second uses \(\{\xNet^{(2)}, \vNet^{(2)}\}\), \(\ldots\), and
the final step uses \(\{\xNet^{(\Nlf)}, \vNet^{(\Nlf)}\}\)).
%
While this approach dramatically increases the number of trainable parameters,
it is also vastly more expressive and allows the model to learn independent
updates for each leapfrog step.
%
\begin{figure}[htpb]
  \centering
  \begin{subfigure}[htpb]{0.3\textwdith}
    \includegraphics[width=\textwidth]{updates_2020_09_22/loss.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}[htpb]{0.3\textwdith}
    \includegraphics[width=\textwidth]{updates_2020_09_22/dq.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}[htpb]{0.3\textwdith}
    \includegraphics[width=\textwidth]{updates_2020_09_22/dq_sin.png}
  \end{subfigure}
  \caption{Loss \(\mathcal{L}(\theta)\), and tunneling rates, \(\delta
  \mathcal{Q}, \delta\mathcal{Q}_{\sin}\) vs training step}
\end{figure}
%
% And for a complete trajectory consisting of \(\Nlf\) leapfrog steps, the above process is repeated
% We can write this update procedure using the single-step leapfrog operator,
% \(\FLq \xi \coloneqq \FLq(x, v, d) = (\xp, \vp, -d) = \xip\).
% %
% For a trajectory consisting of \(\Nlf\) leapfrog steps, we can write a complete
% L2HMC update step as
%
%
% \begin{enumerate}
%   \item \(v(t) \rightarrow v^{\frac{1}{2}} = v(t+\frac{\varepsilon}{2})\)
%   \item \(x \rightarrow x^{\frac{1}{2}} = m^{t} \odot x(t) + \bar{m}^{t} \odot x(t+\varepsilon)\)
%   \item \(x \rightarrow \bar{m}^{t} \odot x(t+\varepsilon) + m^{t} \odot x(t+\varepsilon)\)
% \end{enumerate}
%\end{document}
% \subsubsection{Results:}
\begin{figure}[htpb]
  \centering
  \begin{subfigure}[htpb]{\textwidth}
    \includegraphics[width=\textwidth]{updates_2020_09_22/sep_nets_results/dq_.png}
    % \caption{\(\langle \delta \mathcal{Q}\rangle\) vs training step}
  \end{subfigure}
  % \vspace{2cm}
  \begin{subfigure}[htpb]{\textwidth}
    \includegraphics[width=\textwidth]{updates_2020_09_22/sep_nets_results/dq_sin_.png}
    % \caption{\(\langle \delta \mathcal{Q_{\sin}}\rangle\) vs training step}
  \end{subfigure}
  \caption{Tunneling rates, \(\delta \mathcal{Q}, \delta\mathcal{Q}_{\sin}\)
  vs training step}
\end{figure}
%
\begin{figure}[htpb]
  \centering
  \includegraphics[width=\textwidth]{updates_2020_09_22/sep_nets_results/loss.png}
  \caption{Loss, \(\mathcal{L}(\theta)\) vs training step} 
\end{figure}
%
\begin{figure}[htpb]
  \centering
  \includegraphics[width=\textwidth]{updates_2020_09_22/sep_nets_results/accept_prob.png}
  \caption{Acceptance probability, \(A(\xip|\xi)\) vs training step} 
\end{figure}
%

\clearpage
\subsection{Convolutional Structure / Network Diagrams}
For completeness, we include below the two possible network diagrams for a
single \(\xNet\).

\begin{figure}[htpb]
  \centering
  \includegraphics[width=\textwidth]{updates_2020_09_22/xnet.png}
  \caption{Network diagram for generic \(\xNet\) (without convolutional
  structure).}
\end{figure}
%
\begin{figure}[htpb]
  \centering
  \includegraphics[width=\textwidth]{updates_2020_09_22/xnet_conv.png}
  \caption{Network diagram for convolutional \(\xNet\).}
\end{figure}
