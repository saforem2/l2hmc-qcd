\section{Hamiltonian Monte Carlo}%
\label{sec:l2hmc_hmc}
We can improve upon the random-walk guess and check strategy of the generic
Markov Chain Monte Carlo algorithm by ``guiding'' the simulation according to
the systems natural dynamics using a method known as Hamiltonian (Hybrid)
Monte Carlo (HMC).

In HMC, model samples can be obtained by simulating a physical system
governed by a Hamiltonian comprised of kinetic and potential energy functions
that govern a particles dynamics.
%
By transforming the density function to a potential energy function and
introducing the auxiliary momentum variable $v$, HMC lifts the target
distribution onto a joint probability distribution in phase space $(x, v)$,
where $x$ is the original variable of interest (e.g.\ position in Euclidean
space).
%
A new state is then obtained by solving the equations of motion for a fixed
period of time using a volume-preserving integrator (most commonly the
\emph{leapfrog integrator}).
%
The addition of random (typically normally distributed) momenta encourages
long-distance jumps in state space with a single Metropolis-Hastings (MH)
step.

Let the `position' of the physical state be denoted by a vector $x
\in\mathbb{R}^{n}$ and the conjugate momenta of the physical state be denoted
by a vector $v \in\mathbb{R}^{n}$.
%
Then the Hamiltonian reads
%
\begin{align}
  H(x, v) &= U(x) + K(v)\\
                    & = U(x) + \frac{1}{2} v^{T} \,v,
  \label{eq:hamiltonian}
\end{align}
%
where $U(x)$ is the potential energy, and $K(v)=\frac{1}{2}v^{T}v$ the
kinetic energy.%
%
We assume without loss of generality that the position and momentum variables
are independently distributed.
%
That is, we assume the target distribution of the system can be written as
$\pi(x, v) = \pi(x) \pi(v)$.
%
Further, instead of sampling $\pi(x)$ directly, HMC operates by sampling from the
canonical distribution $\pi(x, v) = \frac{1}{Z} \exp(-H(x, v)) = \pi(x)
\pi(v)$, for some partition function $Z$ that provides a normalization factor.
%
Additionally, we assume the momentum is distributed according to an
identity-covariance Gaussian given by $\pi(v) \propto \exp{(-\frac{1}{2} v^{T} \,
v)}$ For convenience, we will denote the combined state of the system by $\xi
\equiv (x, v)$.
%
From this augmented state $\xi$, HMC produces a proposed state $\xi^{\prime} =
(x^{\prime}, v^{\prime})$ by approximately integrating Hamiltonian dynamics
jointly on $x$ and $v$.
%
This integration is performed along approximate iso-probability contours of
$\pi(x, v) = \pi(x) \pi(v)$ due to the Hamiltonians energy conservation.
%
\subsection{Hamiltonian Dynamics}%
\label{subsec:mcmc_hamiltonian_dynamics}
One of the characteristic properties of Hamilton's equations is that they
conserve the value of the Hamiltonian.
%
Because of this, every Hamiltonian trajectory is confined to an energy
\emph{level set},
%
\begin{equation}
H^{(-1)}(E) = \{x, v | H(x, v) = E\}.
\end{equation}
%
Our state $\xi = (x, v)$ then proceeds to explore this level set by integrating
Hamilton's equations, which are shown as a system of differential equations in
Eq.~\ref{eq:hamiltons_equations}.
% along wich the state $\xi = (x, v)$
% The state $\xi$ is modified in such a way that $H(\xi)$ remains constant thorughout the simulation.
%
% The differential equations governing the motion through state space are given by
%
\begin{align}
  \dot x_i &= \frac{\partial H}{\partial v_i} = v_i\\
  \dot v_i &= -\frac{\partial H}{\partial x_i} = - \frac{\partial
      U}{\partial x_i}
\label{eq:hamiltons_equations}
\end{align}
%
It can be shown~\cite{Neal_2012} that the above transformation is
volume-preserving and reversible, two necessary factors to guarantee asymptotic
convergence of the simulation to the target distribution.
%
The dynamics are simulated using the leapfrog integrator, which for a single
time step consists of:
%
\begin{align}
  v^{\frac{1}{2}} &= v - \frac{\eps}{2} \partial_x U(x)\\
  x^{\prime} &= x + \eps v^{\frac{1}{2}}\\
  v^{\prime} &= v - \frac{\eps}{2} \partial_x U(x^{\prime}).
  \label{eq:generic_leapfrog}
\end{align}
%
We write the action of the leapfrog integrator in terms of an operator
$\mathbf{L}: \mathbf{L}\xi \equiv \mathbf{L}(x, v) \equiv (x^{\prime},
v^{\prime})$, and introduce a momentum flip operator $\mathbf{F}: \mathbf{F}(x,
v) \equiv (x, -v)$.
%
The Metropolis-Hastings acceptance probability for the HMC proposal is given
by:
%
\begin{equation}
  A(\mathbf{F}\mathbf{L} \xi | \xi) = \min\left(1,
      \frac{\pi(\mathbf{F}\mathbf{L}\xi)}{\pi(\xi)}\left|
      \frac{\partial\left[\mathbf{F}\mathbf{L}\xi\right]}
      {\partial\xi^{T}}\right|\right),
\label{eq:metropolis_hastings}
\end{equation}
%
Where $\left|\frac{\partial\left[\mathbf{F}\mathbf{L}\xi\right]}
{\partial\xi^{T}}\right|$ denotes the determinant of the Jacobian describing
the transformation, and is equal to $1$ for traditional HMC.\@
%
In order to utilize these Hamiltonian trajectories to construct an efficient
Markov transition, we need a mechanism for introducing momentum to a given
point in the target parameter space.

Fortunately, this can be done by exploiting the probabilistic structure of the
system~\cite{Betancourt_2017}.
%
To lift an initial point in parameter space into one on phase space, we simply
sample from the conditional distribution over the momentum,
%
\begin{equation}
v \sim \pi(x | v).
\end{equation}
%
Sampling the momentum directly from the conditional distribution ensures that
this lift will fall into the typical set in phase space.
%
We can then proceed to explore the joint typical set by integrating Hamilton's
equations as demonstrated above to obtain a new configuration $\xi \rightarrow
\xip$.
%
We can then return to the target parameter space by simply projecting away the
momentum,
%
\begin{equation}
(x, v) \rightarrow x
\end{equation}
%
These three steps when performed in series gives a complete Hamiltonian Markov
transition composed of random trajectories that rapidly explore the target
distribution, as desired.
%
An example of this process can be seen in Fig~\ref{fig:hmc_phase_space}.
%
\begin{figure}[htpb]
\includegraphics[width=\textwidth]{new_figures/hmc_phase_space11}
\caption{\emph{Visualizing HMC for a $1D$ Gaussian} (example
  from~\cite{Betancourt_2017}, figure adapted with permission
  from~\cite{joeyl2hmc}). Each Hamiltonian Markov transition lifts the
  initial state onto a \color{gray}{random level set of the Hamiltonian,
  }\color{black} $H^{(-1)}(E)$, which can then be explored with a
  \color{blue}{Hamiltonian trajectory }\color{black} before
  \color{red}{projecting back down }\color{black} to the \color{green}{target
  parameter space}\color{black}.}%
\label{fig:hmc_phase_space}
\end{figure}
%
\vspace{-10pt}
\subsubsection{Properties of Hamiltonian Dynamics}
%
There are three fundamental properties of Hamiltonian dynamics which are
crucial to its use in constructing Markov Chain Monte Carlo updates.
%
\begin{enumerate}
  \item \textbf{Reversibility:} Hamiltonian dynamics are \textit{reversible}
      --- the mapping from $\mathbf{L}: \xi(t) \rightarrow \xi^{\prime} =
      \xi(t + s)$ is one-to-one, and consequently has an inverse
      $\mathbf{L}^{-1}$, obtained by negating the time derivatives in
      Eq.~\ref{eq:hamiltons_equations}.
  \item \textbf{Conservation of the Hamiltonian:} Moreover, the dynamics
      \textit{keeps the Hamiltonian invariant}.
  \item \textbf{Volume preservation:} The final property of Hamiltonian
      dynamics is that it \textit{preserves volume} in $(x, v)$ phase space
      (i.e. Liouville's Theorem).
\end{enumerate}
%
All in all, HMC offers noticeable improvements compared to the `random-walk'
approach of generic MCMC, but tends to perform poorly on high-dimensional
distributions.
%
This becomes immediately apparent when it is used for simulations in lattice
gauge theory and lattice QCD, where large autocorrelations and slow `burn-in'
can become prohibitively expensive.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
