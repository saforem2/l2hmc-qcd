\clearpage
\begin{appendices}
  % In Appendix~\ref{sec:l2hmc_hmc}, we include an overview of the Hamiltonian
  % Monte Carlo algorithm for those who may be unfamiliar.
In Appendices~\ref{sec:l2hmc_modified_network}-\ref{sec:topological_loss}, we
describe some of the additional work that has been done in an effort to
improve the algorithms performance when applied to models in lattice gauge
theory and lattice QCD.\
%
% Appendices~\ref{sec:l2hmc_modified_network}-\ref{sec:topological_loss} below
Unfortunately, neither of these approaches seemed to significantly improve
the quality of the sampler nor eliminate the error present in the average
plaquette. 
%
Since each of these new ideas only further complicate the situation, they
have been (temporarily) put on hold until the issues with the average
plaquette can be resolved.
\section{Modified Network Architecture}%
\label{sec:l2hmc_modified_network}
%
In order to better account for the rectangular geometry of the lattice, the
previously described architecture was modified to include a stack of
convolutional layers immediately following the input layer, as shown in
Fig~\ref{fig:conv_net}.
%
The output from this convolutional structure is then fed to the generic network
shown in Fig.~\ref{fig:generic_net}.
%
This is a natural direction to pursue given the inherent translational
invariance of both  convolutional neural networks and rectangular lattices
(with periodic boundary conditions).
%
\begin{figure}[htpb]
\centering
\includegraphics[width=\textwidth]{conv_net/conv_net_final.png}
% \includegraphics[width=\textwidth]{full_network/generic_net.png}
\caption{Convolutional structure used for learning localized features of
rectangular lattice.}% 
\label{fig:conv_net} 
\end{figure}
% \vspace{-40pt}
%
\begin{figure}[htpb]
\centering
\includegraphics[width=0.95\textwidth]{conv_net/vnet_hq.png}
\caption{Illustration taken from TensorBoard showing an overview of the
network architecture for VNet. Note that the architecture is identical for
XNet.} \vspace{12pt}
\includegraphics[width=0.95\textwidth]{conv_net/vnet_zoom_hq.png}
\caption{Detailed view of additional convolutional structure included to better
account for rectangular geometry of lattice inputs.}
\end{figure}
%
Additionally, the network architecture was modified to include a batch
normalization layer after the second MaxPool layer.
%
Introducing batch normalization is a commonly used technique in practice and
is known to:
\begin{enumerate}
\item Help prevent against diverging gradients\footnote{a numerical issue in
    which infinite values are generated when calculating the gradients in
  backpropagation}, (an issue that was occasionally encountered during the
  training procedure).
\item Generally improve model performance by achieving similar performance in
  fewer training steps when compared to models trained without
  it~\cite{Ioffe_Szegedy_2015}.
\end{enumerate}

Additionally, it has been shown to improve model performance and generally
requires fewer training steps to achieve similar performance as models trained
without it~\cite{Ioffe_Szegedy_2015}.

\subsection{Network Diagrams}
For completeness, we include below the two possible network diagrams for a
single \(\xNet\).

\begin{figure}[htpb]
  \centering
  \includegraphics[width=\textwidth]{updates_2020_09_22/xnet.png}
  \caption{Network diagram for generic \(\xNet\) (without convolutional
  structure).}
\end{figure}
%
\begin{figure}[htpb]
  \centering
  \includegraphics[width=\textwidth]{updates_2020_09_22/xnet_conv.png}
  \caption{Network diagram for convolutional \(\xNet\).}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Topological Loss Term}%
\label{sec:topological_loss}
While the alternative metric introduced in Eq.~\ref{eq:new_metric} helps to
better measure distances in this configuration space, it does nothing to
encourage the exploration of different topological sectors since there may be
configurations for which $\delta(\xi, \xip) \approx 1$ but $Q(\xi) = Q(\xip)$.
%
In order to potentially address this issue, we consider the following
modification to the original loss function.

First define $\xip\equiv \FLq \xi$ as the resultant configuration proposed by
the augmented leapfrog integrator, and
%
\begin{align}
\delta_{Q}{(\xi, \xi^{\prime})} &= \left|Q{(\xi)} - Q{(\xi^{\prime})}\right| \\
\ell_{Q}{\left(\xi, \xip, A{(\FLq\xi|\xi)}\right)} &= \delta_{Q}{(\xi,\xip)}
  \times A{(\xip|\xi)}.
\end{align}
%
So we have that $\delta_{Q}$ measures the difference in topological charge
between the initial and proposed configurations, and $\ell_{Q}$ gives the
expected topological charge difference.
%
Proceeding as before, we include an additional auxiliary term which is
identical in structure to the one above, except the input is now a
configuration of link variables $\phi_{\mu}$ drawn from the initialization
distribution $q$, which for our purposes was chosen to be the standard random
normal distribution on $[0, 2\pi$. %]

We can then write the topological loss term as
%
\begin{equation}
\mathcal{L}_{Q}(\theta) \equiv
  \mathbb{E}_{p(\xi)}{\left[\ell_{Q}{\left(\xi,
    \FLq \xi, A{(\FLq\xi|\xi)}\right)}\right]}
    + \alpha_{\mathrm{aux}}\, \mathbb{E}_{q(\xi)}{\left[\ell_{Q}{\left(\xi,
    \FLq \xi, A{(\FLq\xi|\xi)}\right)}\right]}
    \label{eq:topological_loss_term1}
\end{equation}
%
If we denote the standard loss (with the modified metric function) defined in
Eq.~\ref{eq:loss_L} as $\mathcal{L}_{\mathrm{std}}{(\theta)}$, we can write the
new total loss as a combination of these two terms,
%
\begin{equation}
\mathcal{L}(\theta) =
  \alpha_{\mathrm{std}}\, \mathcal{L}_{\mathrm{std}}(\theta) 
  + \alpha_{Q}\, \mathcal{L}_{Q}(\theta)
  \label{eq:topological_loss_term}
\end{equation}
%
where $\alpha_{\mathrm{std}}, \alpha_Q$ are multiplicative factors that weigh
the relative contributions to the total loss from the standard and topological
loss terms respectively, and $\alpha_{\mathrm{aux}}$ in
Eq.~\ref{eq:topological_loss_term1} weighs the contribution of configurations
drawn from the initialization distribution.
% \end{appendix}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\clearpage
\end{appendices}
