%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CCONCLUSION {{{

% Also, as expected, when ran for the same number of leapfrog steps and
% everything else being the same, the difference between the observed and
% expected value of $\langle\phi_{P}\rangle$ increases as $\alpha_{Q}: 0
% \rightarrow 1$.
%
% As Xiao-Yong suggested that it might be due to precision issues or round-off
% errors being carried through, I tried re-running everything using
% \texttt{float64} instead of \texttt{float32} but it didn't seem to make any
% difference, $\langle\phi_{P}\rangle$ still failed to converge to the expected
% value.

% As an alternative, I've also tried replicating te $\ell$ term from the
% original loss function Eq.~\ref{eq:loss_ell} exactly with $\delta(\xi,\xip)
% \rightarrow \delta_{Q}(\xi,\xip)$ but this proved to be very unstable and the
% network was unable to get more than a few training steps in before diverging.

% Additionally, I tried repeating the training/evaluation process using
% multiple different values of both $N_{\mathrm{LF}}$ and $\alpha_Q$
% ($\alpha_Q$ defined in
% Eq.~\ref{eq:topological_loss_term}).
%
% Strangely, the issue seems to be almost irreproducible and doesn't seem to
% follow a clear dependence on either
% $N_{\mathrm{LF}}$ or $\alpha_Q$.
%
% I've included plots illustrating the difference $\delta_{\phi_{P}}$ vs. MD
% step\footnote{Note that each molecular
% dynamics (MD) step consists of a momentum refreshment followed by
% $N_{\mathrm{LF}}$ leapfrog steps, and finally a Metropolis-Hastings
% accept/reject.} below obtained using both the trained L2HMC sampler as well
% as the generic HMC sampler for comparison.


% ---------------------------------------------------------------------------------------------------------------------
% In particular, by drawing a series of correlated samples from a well-defined
% proposal, our collection of samples will converge in distribution to the
% target equilibrium distribution $\pi$, a technique known as Markov Chain
% Monte Carlo (MCMC) sampling.
% % MCMC methods are asymptotically unbiased and remain applicable even in
% cases when we are not able to sample from $\pi$ directly, making them a
% versatile class of algorithms that are widely used across a variety of
% scientific disciplines.
% % While MCMC methods undoubtedly provide an incredibly useful set of tools
% for
% ---------------------------------------------------------------------------------------------------------------------
% }}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CONCLUSION / ISSUES WITH AVERAGE PLAQUETTE {{{
% we denote by $\phi_{P}^{(\mathrm{obs})}$ the \emph{observed} value of
% the average plaquette $\phi_{P}$ and $\phi_{P}^{(\mathrm{exp})}$ the
% \emph{expected} value, we can define the quantity
%
% After many unsuccessful attempts and determining the cause of this behavior,
% I'm still not entirely sure what is causing it.
%
% The difference between the observed value and the expected value also seems
% to increase as more leapfrog steps are performed per MD update as well.


% and allows us to quantify the improvements achieved by alternative
% sampling methods.

% gained as we seek-out and
% identify new met and as we seek-out more efficient sampling methods, this
% allow
%
%
% an important metric compared this to the autocorrelations of samples obtained
% from traditional HMC.
% % I
%
%
% , where it was found that the L2HMC algorithm significantly outperformed
% generic HMC.
% %
%
% sampling for the case of the two-dimensional Gaussian Mixture Model,\@ as
% demonstrated by the trained samplers
% ability to tunnel between modes successfully, as well as the drastic
% improvement in the autocorrelation spectrum.  %
% All in all, the approach outlined in this chapter shows promise in improving
% the efficiency of HMC samplers, particularly within the context of lattice
% gauge theory and lattice QCD.
%
% In this chapter, we have motivated the successfully introduced  the presented
% a learned inference architecture that generalizes HMC by augmenting the
% traditional leapfrog integrator with a set of carefully-chosen functions
% which are parameterized by weights in a neural network.
%
% This system is then trained to maximize the (suitably chosen) `distance'
% between successive samples, leading to
%
%
% All things considered, being able to efficiently sample from high-dimensional
% distributions remains at the heart of every MCMC simulation.
%
% allowing for an independent allowing us to gain insight about mechanisms that
% would otherwise us insight into the mechanisms driving the behavior of the
% world around us.

% often calling upon the worlds largest supercomputers.
%
% calling upon the worlds largest supercomputers to be carried out.

% due to the exponential increase in computational resources resulting from the
% \emph{critical slowing down} as we
% approach the continuum limit through finer and finer lattice spacings.
%



% and the pursuit of better, more efficient algorithms has led to the discovery
% of new ways of
% thinking about of the Being able to sample efficiently from high-dimensional
% distributions remains one of the primary goals
% simulation.
%
% However, for most distributions of interest, exact posterior inference
% remains intractable and as a result, approximate
% statistical inference models become necessary.
%

% One family of such algorithms, Markov Chain Monte Carlo (MCMC), allows us to
% d One of the most well-known examples of this type of algorithm is Arguably
% the most well-known and widely used of which being Markov chain Monte Carlo
% (MCMC) algorithms


% lattice gauge theory and lattice QCD, but

% of gauge generation in lattice gauge theory and lattice QCD.
%
%
% generalized the Hamiltonian Monte Carlo algorithm by augmenting the
% traditional
% leapfrog integrator with a set of carefully-chosen functions which are
% parameterized by weights in a neural network.
%
% These parameters are trained to encourage fast mixing and convergence to the
% desired target distribution.
%
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Systematic Debugging Results}% \label{subsec:debugging_results}
% In each of the plots below, the L2HMC sampler was trained for $25,000$ steps
% with a simulated annealing schedule beginning at $\beta = 2.0$ and ending at
% $\beta = 5.0$.  %
% Additionally, the training was distributed across $16$ nodes on COOLEY using
% \texttt{horovod}.  % The results are averaged over all $N_{\mathrm{samples}}$
% samples in the
% mini-batch.% % \begin{figure}[htpb]\label{fig:plaq_diff_plots_lf5}
%   \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf5_beta5.eps}
%   \hfill
%   \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf5_beta5_HMC.eps}
%   %
%   \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf5_beta6.eps}
%   \hfill
%   \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf5_beta6_HMC.eps}
%   \caption{$\plaqdiff$ vs MD step with $N_{\mathrm{LF}} = 5$ for $\beta =
%   5.0$ (top row) and $\beta = 6.0$ (bottom row). The results from the trained
%   L2HMC (generic HMC) sampler are shown in the left (right) column. As can be
%   seen,
%     the difference $\delta_{\phi_{P}}$ remains roughly consistent for all
%     values of $\alpha_Q$.} \end{figure} %
% \begin{figure}[htpb]\label{fig:plaq_diff_plots_lf6}
% \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf6_beta5.eps}
% \hfill
%   \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf6_beta5_HMC.eps}
%   %
%   \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf6_beta6.eps}
%   \hfill
%   \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf6_beta6_HMC.eps}
%   \caption{$\plaqdiff$ vs MD step with $N_{\mathrm{LF}} = 5$ for $\beta =
%   5.0$ (top row) and $\beta = 6.0$ (bottom
%     row). The results from the trained L2HMC (generic HMC) sampler are shown
%     in the left (right) column. As can be seen, the difference
%     $\delta_{\phi_{P}}$ remains roughly consistent for all values of
%     $\alpha_Q$.} \end{figure}
% % \begin{figure}[htpb]\label{fig:plaq_diff_plots_lf7}
% \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf7_beta5.eps}
% \hfill
% \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf7_beta5_HMC.eps}
%   %
%   \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf7_beta6.eps}
%   \hfill
%   \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf7_beta6_HMC.eps}
%   \caption{$\plaqdiff$ vs MD step with $N_{\mathrm{LF}} = 7$ As can be seen,
%   the difference $\delta_{\phi_{P}}$ is noticeably larger for $\alpha_Q =
%   0.75$, but remains roughly consistent for all other values of $\alpha_Q$.}
% \end{figure} % \begin{figure}[htpb]\label{fig:plaq_diff_plots_lf8_beta5}
%   \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf8_beta5.eps}
%   \hfill
%   \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf8_beta5_HMC.eps}
%   %
%   \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf8_beta6.eps}
%   \hfill
%   \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf8_beta6_HMC.eps}
%   \caption{$\plaqdiff$ vs. MD step with $N_{\mathrm{LF}} = 8$. As can be
%   seen, the difference $\delta_{\phi_{P}}$ remains roughly consistent for all
%   values of $\alpha_Q$.}
% \end{figure} % \begin{figure}[htpb]\label{fig:plaq_diff_plots_lf9}
%   \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf9_beta5.eps}
%   \hfill
%   \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf9_beta5_HMC.eps}
%   %
%   \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf9_beta6.eps}
%   \hfill
%   \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf9_beta6_HMC.eps}
%   \caption{$\plaqdiff$ vs MD step with $N_{\mathrm{LF}} = 9$. As can be seen,
%   the difference $\delta_{\phi_{P}}$ is largest for $\alpha_Q = 0.0$ and
%   $\alpha_Q = 0.5$.}
% \end{figure} % \begin{figure}[htpb]\label{fig:plaq_diff_plots_lf10}
%   \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf10_beta5.eps}
%   \hfill
%   \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf10_beta5_HMC.eps}
%   %
%   \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf10_beta6.eps}
%   \hfill
%   \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf10_beta6_HMC.eps}
%   \caption{$\plaqdiff$ vs MD step with $N_{\mathrm{LF}} = 10$. As can be
%   seen, the difference $\delta_{\phi_{P}}$ is smallest for $\alpha_Q =
%   0.,\,\,0.75$ and largest for $\alpha_Q = 0.25$, while $\alpha_Q =
%   0.5,\,\,2.0$, takes on
%   intermediate values.} \end{figure} %
% \begin{figure}[htpb]\label{fig:plaq_diff_plots_lf15}
% \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf15_beta5.eps}
% \hfill
%   \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf15_beta5_HMC.eps}
%   %
%   \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf15_beta6.eps}
%   \hfill
%   \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf15_beta6_HMC.eps}
%   \caption{$\plaqdiff$ vs MD step with $N_{\mathrm{LF}} = 15$. As can be
%   seen, the difference $\delta_{\phi_{P}}$
%     varies for different values of $\alpha_Q$.} \end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% I'm currently working on re-running all of the following experiments to see
% if this difference $\plaqdiff$ is reproducible or if it is some sort of
% anomaly.

% \subsubsection*{Diverging gradients} On a separate `debugging' note,
% previously I had mentioned that I would occasionally run into an issue where
% the
% gradients would (seemingly randomly) blow up during training. I had spoken
% with Xiao-Yong about this in a little more depth and I explained that I
% believed it was directly related to the gradient of the step size, which I
% was able to occasionally avoid by introducing a gradient clipping operation
% during backpropagation. This approach seemed to work
% well in some cases but would still occasionally diverge.
%
%
% \subsection*{Topological susceptibility \texorpdfstring{$\chi$}{χ:}} %
% \begin{figure}[htpb]
%   \centering
%   \includegraphics[width=0.95\textwidth]{suscept_plots/topological_suscept_vs_beta_all_10}
%   \caption{Topological susceptibility vs. $\beta$ for various lattice sizes
%   $L = 8, 12, 16$. Note that the data points in the inset plots are slightly
%   shifted horizontally from each other for illustrative purposes.}%
%   \label{fig:suscept}
% \end{figure} % \begin{itemize}
%   \item All statistics for $\chi$ were calculated by running either the
%   trained L2HMC sampler or a generic HMC sampler with equivalent parameters
%   (step size at the end of training, number of leapfrog steps,
%     etc.).  % \item For $N_{\mathrm{MD}}$ molecular dynamics (accept/reject)
%     update steps for $N_{\mathrm{chains}}$ (batch size) chains in parallel.
%     %
%   \item Each molecular dynamics update step consists of $5$ individual
%   leapfrog steps (augmented leapfrog step for L2HMC sampler).  % \item For
%   the results shown in Fig.~\ref{fig:suscept}, $N_{\mathrm{MD}} =
%     1\times10^4$ for all values of $L$, and $N_{\mathrm{chains}} = 32, 50,
%     64$ for $L = 16, 12$, and $8$ respectively.  \end{itemize}
% % \begin{figure}[htpb]%\label{fig:top_charges8_l2hmc} \centering
%   \includegraphics[width=0.49\textwidth]{charge_plots/top_charge_vs_step8_L2HMC}
%   \hfill
%   \includegraphics[width=0.49\textwidth]{charge_plots/top_charge_vs_step8_HMC}
%   \includegraphics[width=0.49\textwidth]{charge_plots/top_charge_vs_step12_L2HMC}
%   \hfill
%   \includegraphics[width=0.49\textwidth]{charge_plots/top_charge_vs_step12_HMC}
%   \includegraphics[width=0.49\textwidth]{charge_plots/top_charge_vs_step16_L2HMC}
%   \hfill
%   \includegraphics[width=0.49\textwidth]{charge_plots/top_charge_vs_step16_HMC}
%   \caption{Topological charge of a single chain vs.\ evaluation step at
%   $\beta=5$. Top row: $L=8$, Middle row: $L=12$, bottom row $L=16$. Left
%   column: trained L2HMC sampler, right column: generic HMC sampler}
% \end{figure} % \begin{figure}[htpb]%\label{}
%   \centering
%   \includegraphics[width=0.49\textwidth]{charge_plots/top_charge_vs_step8_b4_L2HMC}
%   \hfill
%   \includegraphics[width=0.49\textwidth]{charge_plots/top_charge_vs_step8_b4_HMC}
%   \includegraphics[width=0.49\textwidth]{charge_plots/top_charge_vs_step12_b4_L2HMC}
%   \hfill
%   \includegraphics[width=0.49\textwidth]{charge_plots/top_charge_vs_step12_b4_HMC}
%   \includegraphics[width=0.49\textwidth]{charge_plots/top_charge_vs_step16_b4_L2HMC}
%   \hfill
%   \includegraphics[width=0.49\textwidth]{charge_plots/top_charge_vs_step16_b4_HMC}
%   \caption{Topological charge of a single chain vs.\ evaluation step at
%   $\beta=4$. Top row: $L=8$, Middle row: $L=12$, bottom row $L=16$. Left
%     column: trained L2HMC sampler, right column: generic HMC sampler}
%     \end{figure}
%
% \section{UPDATES (04/22/2019) Autocorrelation of topological charge
% \texorpdfstring{$Q$}{Q}} We are interested in the autocorrelation of the
% topological charge $Q$.  %
% \begin{figure}[htpb] \centering
% \includegraphics[width=0.48\textwidth]{autocorrelations/top_charge_autocorrelations_L8_steps3_batch256}
%   \includegraphics[width=0.48\textwidth]{autocorrelations/top_charge_autocorrelations_L8_steps5_batch128}
%   \caption{Autocorrelation of the topological charge $Q$, on an $8 \times 8$
%   lattice, averaged over $128$ samples. The molecular dynamics
%   update consisted of $3$ (left) and $5$ (right) augmented (L2HMC) leapfrog
%   steps.} \label{fig:charge_acorr_L8} \end{figure}
% % \begin{figure}[htpb] \centering
%   \includegraphics[width=0.48\textwidth]{autocorrelations/top_charge_autocorrelations_L16_steps3_batch256}
%   \includegraphics[width=0.48\textwidth]{autocorrelations/top_charge_autocorrelations_L16_steps5_batch128}
%   \caption{Autocorrelation of the topological charge $Q$, on an $16 \times
%   16$
%     lattice, averaged over $128$ samples. The molecular dynamics update
%     consisted of $3$ (left) and $5$ (right) leapfrog steps.}
%     \label{fig:charge_acorr_L16}
% \end{figure}
%
% The model was trained using simulated annealing for $N_{\mathrm{train}}$
% training steps.
% % The paramter $\beta$ was updated according to the annealing schedule shown
% in Eq.~\ref{eq:annealing_schedule}.  %
% \begin{equation} \frac{1}{\beta(n)} = {\left(\frac{1}{\beta_{i}} -
% \frac{1}{\beta_{f}}\right)} {\left(\frac{1 - n}{N_{\mathrm{train}}}\right)} +
% \frac{1}{\beta_{f}}
%     \label{eq:annealing_schedule} \end{equation} %
% Here $\beta(n)$ denotes the value of beta to be used for the
% $n^{\mathrm{th}}$ training step ($n = 1, \ldots, N_{\mathrm{train}}$),
% $\beta_{i}$ represents the initial value of $\beta$ at the beginning of the
% training, and $\beta_{f}$ represents the final value of $\beta$ at the end of
% training.
% % For all of the exmaples above, $N_{\mathrm{train}} = 25,000$, $\beta_{0} =
% 2$ and $\beta_{N_{\mathrm{train}}} = 5$.

% \section{UPDATES (05/03/2019)} Previously we had noticed that there seems to
% be a discrepancy between the observed and expected value of the average
% plaquette, % \begin{equation}
%   {\delta_{\phi_P}(\alpha_Q, N_{\mathrm{LF}}) \equiv \langle
%   \phi_P^{\mathrm{(obs)}}\rangle - \langle{\phi_{P}^{\mathrm{(exp)}}}\rangle
%   \neq 0}.  \end{equation}
% % In an attempt to better understand why this was occurring, I tried
% repeating the training/evaluation process using multiple different values of
% both $N_{\mathrm{LF}}$ and $\alpha_Q$ ($\alpha_Q$ defined in
% Eq.~\ref{eq:topological_loss_term}).  % Strangely, the issue seems to be
% almost irreproducible and doesn't seem to follow a clear dependence on either
% $N_{\mathrm{LF}}$ or $\alpha_Q$.  % I've included plots illustrating the
% difference $\delta_{\phi_{P}}$ vs. MD step\footnote{Note that each molecular
% dynamics (MD) step consists of a momentum refreshment followed by
% $N_{\mathrm{LF}}$ leapfrog steps, and finally a Metropolis-Hastings
% accept/reject.} below obtained using both the trained L2HMC sampler as well
% as the generic HMC sampler for comparison.
% % In each of the plots below, the L2HMC sampler was trained for $25,000$
% steps with a simulated annealing schedule beginning at $\beta = 2.0$ and
% ending at $\beta = 5.0$.
% % Additionally, the training was distributed across $16$ nodes on COOLEY
% using \texttt{horovod}.  %
% The results are averaged over all $N_{\mathrm{samples}}$ samples in the
% mini-batch.
%
% I'm currently working on re-running all of the following experiments to see
% if this difference $\plaqdiff$ is reproducible or if it is some sort of
% anomaly.
%
%
%
% \include{autocorrelation_plots}
%
% \section{UPDATES (05/15/2019) Debugging convergence issues} We mainly want to
% know: \begin{itemize}
%   \item How the plain link variables, $\phi_{\mu}(i) \in [-\pi, \pi)$ change
%   during the leapfrog updates. If we denote by $\phi_{\mu}^{(t)}{(i)}$ an
%   individual link variable at the $t^{th}$ leapfrog step then we can look at
%   how these values change between subsequent leapfrog updates through the
%   quantity
%     \begin{equation} \delta \phi_{\mu}{(i)} = \phi_{\mu}^{(t+1)}{(i)} -
%     \phi_{\mu}^{(t)}{(i)} \end{equation}
%     For illustrative purposes, we can look at how the average link changes
%     and define \begin{align} \langle \delta \phi_{\mu}{(i)}\rangle &=
%     \frac{1}{N_{\mathrm{links}}}\sum_{\mu, i} \delta \phi_{\mu}(i) \\
%     \end{align} % \item How the determinant of the Jacobian changes during
%     the leapfrog updates. For the augmented leapfrog operator
%     $\lfop$, we can compute the Jacobian $\mathcal{J}$: \begin{align}
%     \mathcal{J} &\equiv \log\left|\frac{\partial
%     {[\mathbf{F}\lfop\xi]}}{\partial\xi^{T}}\right| = \sum_{t \leq
%                       N_{\mathrm{lf}}} \mathcal{J}^{(t)} \\
%                   &= \sum_{t\leq N_{\mathrm{lf}}} \log\left|\frac{\partial
%                       {[\mathbf{F}\lfop\xi^{t}]}}{\partial\xi^{T}}\right| \\
%                   &= d \, \sum_{t \leq M}
%                   \bigg[\frac{\eps}{2}\mathbbm{1}\cdot S_{v}{(\zeta_1^t)} +
%                   \varepsilon m^t \cdot S_x{(\zeta_2^t)} + \varepsilon
%                   \bar{m}^t \cdot S_x{(\zeta_3^t)} +
%                       \frac{\varepsilon}{2}\mathbbm{1}\cdot
%                       S_{v}{(\zeta_4^t)}\bigg] \label{eq:logdet} \end{align}
%                       where $\zeta_i^t$ denotes the intermediary variable
%                       $\zeta_i$
%     at time step $t$ and $d$ is the direction of $\xi$, i.e.\ $\xi = \{x, v,
%     d\}$.  \end{itemize}
%
% Note that in each of the plots below, the faint red (blue) lines represent
% the value for individual samples in our mini-batch, whereas the bold red
% (blue) line indicates the average across all samples.
% % Additionally, for notational simplicity, we define a single MD step to
% consist of $N_{\mathrm{lf}}$ leapfrog steps followed by a Metropolis-Hastings
% accept/reject.
% }}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
