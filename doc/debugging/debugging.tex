\section{Debugging}%
\label{sec:debugging}%
\subsection{Net Weights}
\label{subsec:net_weights}
%
In an attempt to better understand the source of this unexpected behavior,
multiplicative weights (referred to as `nw' for `network weights' in
Fig.~\ref{fig:bad_convergence}) were introduced to scale the individual
contribution from each of the `learned' functions $S$, $T$, and $Q$,
explicitly:
%
\begin{align}
  S &\rightarrow \alpha_{S}\, S \\
  T &\rightarrow \alpha_{T}\, T \\
  Q &\rightarrow \alpha_{Q}\, Q.
\end{align}
%
By varying each of these weights individually allows us to selectively `tune'
how much each of these functions contribute when running inference on the
trained model.
%
Note that in the limit $\alpha_{S}, \alpha_{T}, \alpha_{Q} \rightarrow 0$, we
recover generic HMC, and as can be seen in Fig.~\ref{fig:bad_convergence},
the error in the average plaquette $\delta_{\phi_{P}} \simeq 0$, as expected.
%
As an additional sanity check, we looked at how the error in the average
plaquette behaves for different values of the weights $\alpha_{i}$ ($i = S, T,
Q$).
%
Explicitly, beginning with $\vec{\alpha} \equiv [\alpha_{S}, \alpha_{Q},
\alpha_{T}] = [0, 0, 0]$, we increase each of the weights one by one and compute
the average value of the plaquette difference.
%
For example, the blue line (Transformation $(Q)$ function) in
Fig.~\ref{fig:plaq_diff_vs_net_weights} was obtained by keeping both
$\alpha_{S}$ and $\alpha_{T}$ fixed and $0$ and varying $\alpha_{Q} \in
[0.1, 0.25, 0.5, 0.75, 1.0, 1.5, 2.0, 5.0]$, and similarly for $\alpha_{S}$ and
$\alpha_{Q}$.
%
\begin{figure}
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \caption{$N_{LF} = 10$}
    \includegraphics[width=\textwidth]{new_figures/plaq_error/plaq_diff_vs_net_weights_lf10.pdf}
  \end{subfigure}
  % \vspace{2pt}
  % \hfill
  % \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \caption{$N_{LF} = 12$}
    \includegraphics[width=\textwidth]{new_figures/plaq_error/plaq_diff_vs_net_weights_lf12.pdf}
  \end{subfigure}
  % \vspace{2pt}
  \begin{subfigure}[b]{0.48\textwidth}
    \caption{$N_{LF} = 16$}
    \includegraphics[width=\textwidth]{new_figures/plaq_error/plaq_diff_vs_net_weights_lf16.pdf}
  \end{subfigure}
  \caption{Plaquette difference $\delta_{\phi_{P}}$ for different values of the
    net weights $\vec{\alpha} \equiv [\alpha_{S}, \alpha_{T}, \alpha_{Q}]$.
    Note that $\delta_{\phi_{P}} \rightarrow 0$ as $\vec{\alpha} \rightarrow
  [0, 0, 0]$, as expected.}%
\label{fig:plaq_diff_vs_net_weights}
\end{figure}
%
These results seem to indicate that each of the individual functions contribute
separately to the error, with the scaling ($S$) and translation ($T$) functions
having the largest effect.
%  }}}
\subsection{Updates (11/11/2019)}
\begin{todolist}
  \item[\done] Ensure reproducibility across training/inference runs when
    using same input parameters.
    \begin{itemize}
      \item Essential for debugging the bias in the average plaquette.
      \item Somewhat tricky problem due to the fact that tensorflow 
        has two distinct methods of specifying a seed: graph-level and
        operation-level. Additionally, when using horovod for distributed
        training across multiple ranks, we must ensure that each rank gets a
        different seed otherwise they will all be training identical copies of
        the model.
    \end{itemize}

  \item[\done] Implement reversibility checker that ensures that the L2HMC
    dynamics (i.e.\ the augmented HMC sampler) is reversible.
    \begin{itemize}
      \item Starting with $\xi = (x, v, d)$, run the dynamics in the forward
        direction to get $\xi^{\prime}$. If $\mathbf{L}_{\theta}
        \xi =  \xi^{\prime}$, and $\mathbf{F} \xi = \mathbf{F} (x, v, d) = (x,
        v, -d)$, then a complete (invertible) update step can be written as
        $\mathbf{FL}_{\theta}\xi = \xi^{\prime}$.
        %
      \item If our sampler is reversible, running the dynamics backwards on
        $\xi^{\prime}$ should return us to the original state $\xi$. 
        %
        \begin{equation}
          \mathbf{FL}_{\theta}\mathbf{FL}_{\theta} \xi = \xi
        \end{equation}
    \end{itemize}

  \item[\done] Try increasing floating point precision (\texttt{tf.float32 -->
    tf.float64}) (\textbf{\textcolor{red}{Error still present.}}) 

  \item[\done] Try anti-symmetric Gaussian Mixture Model and see if the trained
    model is an accurate representation of the target distribution (e.g.\ by
    looking at the locations of the means).
    % \href{https://l2hmc.slack.com/archives/CK0SMC6NS/p1567623563026900}{(link
    % to post)}.

  \item[\done] At James' suggestion, we decided to look at the
    kinetic/potential energies and the Hamiltonian at the beginning and end of
    each trajectory (\textbf{\textcolor{red}{Solved! Issue was being caused by
    unexpected resampling of the momentum}}).
    % Fig~\ref{fig:potential_energy},~\ref{fig:kinetic_energy},~\ref{fig:hamiltonian}).
  \end{todolist}
\section{GMM Results}%
\label{sec:gmm_results}
\begin{figure}[htpb] 
  \centering 
  \includegraphics[width=0.5\textwidth]{updates/single_chain}
  \caption{Inference run shown for a single chain using the L2HMC
  sampler trained on this gaussian mixture model.}%
  \label{fig:gmm_single_chain}
\end{figure}

\begin{figure}[htpb] 
  \centering 
  \begin{subfigure}[t]{0.48\textwidth}
    \caption{L2HMC samples}
    \includegraphics[width=\textwidth]{updates/means_hist_observed}
  \end{subfigure}
  % \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \caption{true target distribution}
    \includegraphics[width=\textwidth]{updates/means_hist_true}
  \end{subfigure}
  \caption{Histograms of $\langle x\rangle$ and $\langle y\rangle$ in this
  two-dimensional target space.}
\end{figure}
% }}}
