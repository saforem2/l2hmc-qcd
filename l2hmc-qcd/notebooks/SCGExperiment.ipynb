{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T19:41:35.405767Z",
     "start_time": "2020-03-07T19:41:31.206691Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.func_utils import accept, jacobian, autocovariance, get_log_likelihood, \\\n",
    "    get_data, binarize, normal_kl, acl_spectrum, ESS\n",
    "from utils.distributions import Gaussian, GMM, GaussianFunnel, gen_ring\n",
    "from utils.layers import Linear, Sequential, Zip, Parallel, ScaleTanh\n",
    "from utils.dynamics import Dynamics\n",
    "from utils.sampler import propose\n",
    "from utils.notebook_utils import get_hmc_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture\n",
    "\n",
    "We define the network architecture for our L2HMC network. We first embed the first two variables ($\\{x, \\partial_x U\\}$ or $\\{v, x_m\\}$) as well as the time, formatted as $\\tau(t) = \\left(\\cos(\\frac{2\\pi t}{M}), \\sin(\\frac{2\\pi t}{M})\\right)$. They are forwarded through an MLP and then produce $S, T$ and $Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(x_dim, scope, factor):\n",
    "    with tf.variable_scope(scope):\n",
    "        net = Sequential([\n",
    "            Zip([\n",
    "                Linear(x_dim, 10, scope='embed_1', factor=1.0 / 3),\n",
    "                Linear(x_dim, 10, scope='embed_2', factor=factor * 1.0 / 3),\n",
    "                Linear(2, 10, scope='embed_3', factor=1.0 / 3),\n",
    "                lambda _: 0.,\n",
    "            ]),\n",
    "            sum,\n",
    "            tf.nn.relu,\n",
    "            Linear(10, 10, scope='linear_1'),\n",
    "            tf.nn.relu,\n",
    "            Parallel([\n",
    "                Sequential([\n",
    "                    Linear(10, x_dim, scope='linear_s', factor=0.001), \n",
    "                    ScaleTanh(x_dim, scope='scale_s')\n",
    "                ]),\n",
    "                Linear(10, x_dim, scope='linear_t', factor=0.001),\n",
    "                Sequential([\n",
    "                    Linear(10, x_dim, scope='linear_f', factor=0.001),\n",
    "                    ScaleTanh(x_dim, scope='scale_f'),\n",
    "                ])\n",
    "            ])  \n",
    "        ])\n",
    "        \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution\n",
    "\n",
    "We define our energy function. It is a Gaussian distribution with zero mean. The covariance is a $\\pi/4$ rotation of the eigenvalues $[100, 10^{-1}]$. We set up our dynamics which take as input our energy function, the number of time step of our operator, the (learnable) step-size and our architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim = 2\n",
    "mu = np.zeros(2,)\n",
    "cov = np.array([[50.05, -49.95], [-49.95, 50.05]])\n",
    "\n",
    "distribution = Gaussian(mu, cov)\n",
    "dynamics = Dynamics(x_dim, distribution.get_energy_function(), T=10, eps=0.1, net_factory=network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly sample from this distribution and plot it for sanity-check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = distribution.get_samples(200)\n",
    "plt.title('Strongly Correlated Gaussian (SCG)')\n",
    "plt.scatter(S[:, 0], S[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up the loss on both $p(\\xi)$ (here `x`) and $q(\\xi)$ (here `z`). We then train with Adam with a learning rate of $10^{-3}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None, x_dim))\n",
    "z = tf.random_normal(tf.shape(x))\n",
    "\n",
    "Lx, _, px, output = propose(x, dynamics, do_mh_step=True)\n",
    "Lz, _, pz, _ = propose(z, dynamics, do_mh_step=False)\n",
    "\n",
    "loss = 0.\n",
    "\n",
    "v1 = (tf.reduce_sum(tf.square(x - Lx), axis=1) * px) + 1e-4\n",
    "v2 = (tf.reduce_sum(tf.square(z - Lz), axis=1) * pz) + 1e-4\n",
    "scale = 0.1\n",
    "\n",
    "loss += scale * (tf.reduce_mean(1.0 / v1) + tf.reduce_mean(1.0 / v2))\n",
    "loss += (- tf.reduce_mean(v1) - tf.reduce_mean(v2)) / scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0., name='global_step', trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(1e-3, global_step, 1000, 0.96, staircase=True)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop described in Algorithm $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 5000\n",
    "n_samples = 200\n",
    "\n",
    "samples = np.random.randn(n_samples, x_dim)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for t in range(n_steps):\n",
    "    _, loss_, samples, px_, lr_ = sess.run([\n",
    "        train_op,\n",
    "        loss,\n",
    "        output[0],\n",
    "        px,\n",
    "        learning_rate,\n",
    "    ], {x: samples})\n",
    "\n",
    "    if t % 100 == 0:\n",
    "        print 'Step: %d / %d, Loss: %.2e, Acceptance sample: %.2f, LR: %.5f' % (t, n_steps, loss_, np.mean(px_), lr_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we generate $200$ chains for $2000$ steps for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = distribution.get_samples(n=n_samples)\n",
    "final_samples = []\n",
    "\n",
    "for t in range(2000):\n",
    "    final_samples.append(np.copy(samples))\n",
    "\n",
    "    feed_dict = {\n",
    "        x: samples,\n",
    "    }\n",
    "\n",
    "    samples = sess.run(output[0], feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the HMC chains with **auto-correlation spectrums** as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2HMC_samples = np.array(final_samples)\n",
    "HMC_samples_1 = get_hmc_samples(2, 0.1, distribution.get_energy_function(), sess, steps=2000, samples=samples)\n",
    "HMC_samples_2 = get_hmc_samples(2, 0.15, distribution.get_energy_function(), sess, steps=2000, samples=samples)\n",
    "HMC_samples_3 = get_hmc_samples(2, 0.2, distribution.get_energy_function(), sess, steps=2000, samples=samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = np.sqrt(np.trace(cov))\n",
    "L2HMC = acl_spectrum(L2HMC_samples, scale=scale)\n",
    "HMC1 = acl_spectrum(HMC_samples_1, scale=scale)\n",
    "HMC2 = acl_spectrum(HMC_samples_2, scale=scale)\n",
    "HMC3 = acl_spectrum(HMC_samples_3, scale=scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot auto-correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xaxis = 10 * np.arange(300)\n",
    "plt.plot(xaxis, L2HMC[:300], label='L2HMC')\n",
    "plt.plot(xaxis, HMC1[:300], label='HMC $\\epsilon=0.1$')\n",
    "plt.plot(xaxis, HMC2[:300], label='HMC $\\epsilon=0.15$')\n",
    "plt.plot(xaxis, HMC3[:300], label='HMC $\\epsilon=0.2$')\n",
    "plt.ylabel('Auto-correlation')\n",
    "plt.xlabel('Gradient Computations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the **Effective Sample Size** (ESS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'ESS L2HMC: %.2e -- ESS HMC: %.2e -- Ratio: %d' % (ESS(L2HMC), ESS(HMC2), ESS(L2HMC) / ESS(HMC2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize a single chain of L2HMC for $50$ time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(S[:, 0], S[:, 1])\n",
    "plt.plot(L2HMC_samples[:50, 1, 0], L2HMC_samples[:50, 1, 1], color='orange', marker='o', alpha=0.8)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:tf15] *",
   "language": "python",
   "name": "conda-env-tf15-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
